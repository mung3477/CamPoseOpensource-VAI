import h5py
import torch
import os
import numpy as np
import random
import re
import math
import glob
import json
import einops
from scipy.spatial.transform import Rotation
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms.functional as TF
import torchvision.transforms as T
from cam_embedding import PluckerEmbedder
import torch.nn.functional as F

from eval import to_mp4

# --- Utility Functions ---

def set_seed(seed):
    """Set seed for reproducibility."""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

def compute_dict_mean(dict_list):
    """Compute the mean value for each key in a list of dictionaries."""
    if len(dict_list) == 0:
        return {}
    
    mean_dict = {}
    for key in dict_list[0].keys():
        if not isinstance(dict_list[0][key], torch.Tensor):
            continue  # Skip non-tensor values
        mean_dict[key] = torch.stack([d[key] for d in dict_list]).mean()
    return mean_dict

def detach_dict(dictionary):
    """Detach all tensors in a dictionary."""
    result = {}
    for k, v in dictionary.items():
        if isinstance(v, torch.Tensor):
            result[k] = v.detach()
        else:
            result[k] = v
    return result

def cleanup_ckpt(ckpt_dir, keep=1):
    """Keep only the latest N checkpoints."""
    ckpts = glob.glob(os.path.join(ckpt_dir, "epoch_*.pth"))
    if len(ckpts) <= keep:
        return
    
    epoch_nums = []
    for ckpt in ckpts:
        match = re.search(r"epoch_(\d+).pth", ckpt)
        if match:
            epoch_nums.append((int(match.group(1)), ckpt))
    
    epoch_nums.sort(reverse=True)
    
    for _, ckpt in epoch_nums[keep:]:
        os.remove(ckpt)

def get_last_ckpt(ckpt_dir):
    """Get the latest checkpoint in the directory."""
    if not os.path.exists(ckpt_dir):
        return None
    
    ckpts = glob.glob(os.path.join(ckpt_dir, "epoch_*.pth"))
    if not ckpts:
        return None
    
    latest_epoch = -1
    latest_ckpt = None
    for ckpt in ckpts:
        match = re.search(r"epoch_(\d+).pth", ckpt)
        if match:
            epoch = int(match.group(1))
            if epoch > latest_epoch:
                latest_epoch = epoch
                latest_ckpt = ckpt
    
    return latest_ckpt

def cosine_schedule(optimizer, total_steps, eta_min=0.0):
    """Cosine learning rate schedule."""
    def lr_lambda(step):
        return eta_min + 0.5 * (1 - eta_min) * (1 + math.cos(math.pi * step / total_steps))
    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)

def cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps, eta_min=0.0):
    """Cosine learning rate schedule with warmup."""
    def lr_lambda(step):
        if step < warmup_steps:
            return step / warmup_steps
        return eta_min + 0.5 * (1 - eta_min) * (1 + math.cos(math.pi * (step - warmup_steps) / (total_steps - warmup_steps)))
    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)

def constant_schedule(optimizer):
    """Constant learning rate schedule (no decay)."""
    return torch.optim.lr_scheduler.LambdaLR(optimizer, lambda step: 1.0)


def get_norm_stats(dataset_path, num_demos, policy_class: str = 'dp'):
    """
    Compute normalization statistics for actions and states from robosuite dataset.
    
    Args:
        dataset_path (str): Path to the robosuite HDF5 dataset
        num_demos (int): Number of demonstrations to use for computing stats.
    Returns:
        dict: Dictionary containing normalization statistics
    """
    all_states_data = []
    all_action_data = []
    
    with h5py.File(dataset_path, 'r') as dataset_file:
        demo_keys = [k for k in dataset_file['data'].keys() if k.startswith('demo_')]
        num_demos_available = len(demo_keys)
        num_demos_to_use = min(num_demos, num_demos_available)
            
        print(f"Computing robosuite normalization statistics using {num_demos_to_use} demonstrations from {dataset_path}...")
        
        for i in range(num_demos_to_use):
            demo_key = f'demo_{i}'
            
            # Load states and actions from robosuite format
            states = dataset_file[f'data/{demo_key}/states'][()].astype(np.float32)
            actions = dataset_file[f'data/{demo_key}/actions'][()].astype(np.float32)
            
            # Extract only robot joint positions (first 7 dimensions)
            robot_qpos = states[:, :7]  # Robot joint positions

            all_states_data.append(robot_qpos)
            all_action_data.append(actions)

    states_array = np.concatenate(all_states_data, axis=0)
    actions_array = np.concatenate(all_action_data, axis=0)

    # Use min–max scaling for diffusion policy variants ('dp') so that
    # values lie in approximately [-1, 1], matching the DDPM clip range during sampling.
    if policy_class in ['dp']:
        # Use min–max scaling encoded as mean/std such that
        # (x - mean)/std == 2*(x - min)/(max-min) - 1
        s_min = states_array.min(axis=0)
        s_max = states_array.max(axis=0)
        a_min = actions_array.min(axis=0)
        a_max = actions_array.max(axis=0)

        state_std = np.maximum((s_max - s_min) / 2.0, 1e-6)
        state_mean = (s_min + s_max) / 2.0

        action_std = np.maximum((a_max - a_min) / 2.0, 1e-6)
        action_mean = (a_min + a_max) / 2.0

        print("action max, min:", a_max, a_min)
    elif policy_class == 'pi0':
        # FAST expects per-dimension robust bounds mapped to [-1, 1].
        # Encode via mean/std so that (x - mean)/std reproduces the robust mapping.
        # Actions: use 1st and 99th percentiles per dimension.
        a_q1 = np.quantile(actions_array, 0.01, axis=0)
        a_q99 = np.quantile(actions_array, 0.99, axis=0)
        action_std = np.maximum((a_q99 - a_q1) / 2.0, 1e-6)
        action_mean = (a_q1 + a_q99) / 2.0

        # States: keep z-score (or switch to robust similarly if desired)
        state_mean = np.mean(states_array, axis=0)
        state_std = np.std(states_array, axis=0)
        state_std = np.clip(state_std, 1e-4, np.inf)
        print("[pi0] action q99, q1:", a_q99, a_q1)
    else:
        # Default z-score
        state_mean = np.mean(states_array, axis=0)
        state_std = np.std(states_array, axis=0)
        state_std = np.clip(state_std, 1e-4, np.inf)

        action_mean = np.mean(actions_array, axis=0)
        action_std = np.std(actions_array, axis=0)
        action_std = np.clip(action_std, 1e-4, np.inf)

    stats = {
        "state_mean": state_mean,
        "state_std": state_std,
        "action_mean": action_mean,
        "action_std": action_std,
    }
    
    print(f"State Mean shape: {stats['state_mean'].shape}, Action Mean shape: {stats['action_mean'].shape}")
    return stats

class RGBJitter(object):
    """
    Apply color jittering to the RGB channels (first 3 channels) of the image.
    """
    def __init__(self, brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1):
        self.rgb_jitter = T.ColorJitter(
            brightness=brightness,
            contrast=contrast, 
            saturation=saturation, 
            hue=hue
        )
    
    def __call__(self, img):
        assert img.dim() == 3
        img[:3] = self.rgb_jitter(img[:3])
        return img

class RandomCrop(object):
    def __init__(self, min_side=224, max_side=256, output_size=256):
        self.min_side = min_side
        self.max_side = max_side
        self.output_size = output_size

    def __call__(self, image):
        assert image.dim() == 3
        C, H, W = image.shape
        assert H >= self.min_side and W >= self.min_side
        assert H <= self.max_side and W <= self.max_side

        crop_size = random.randint(self.min_side, self.max_side)
        top = random.randint(0, H - crop_size)
        left = random.randint(0, W - crop_size)
        cropped = image[:, top:top+crop_size, left:left+crop_size]
        resized = TF.resize(cropped, [self.output_size, self.output_size], interpolation=T.InterpolationMode.NEAREST)
        return resized

def save_image_batch_as_mp4(image_batch, save_path):
    rgb_batch = image_batch[:, :3, :, :].cpu().numpy()  # [B, 3, H, W]
    image_list = []
    for i in range(rgb_batch.shape[0]):
        img = rgb_batch[i]  # [3, H, W]
        img = np.transpose(img, (1, 2, 0))  # [H, W, 3]
        img = (img * 255).astype(np.uint8)
        image_list.append(img)
    to_mp4(save_path + '.mp4', image_list)

# --- Dataset Class ---

class EpisodicDataset(Dataset):
    """
    Dataset for loading episodic data from robosuite demonstration files.
    Renders images on-the-fly with dynamic camera poses and Plucker embeddings.
    """
    def __init__(self, demo_indices, norm_stats, args, camera_poses_file=None,
                 max_seq_length=None, transform="id", env=None):
        """
        Args:
            demo_indices (list): List of demonstration indices to use
            norm_stats (dict): Normalization statistics for actions and states
            args: Arguments object containing dataset_path, camera_names, use_plucker, etc.
            camera_poses_file (str): Path to JSON file with camera poses
            max_seq_length (int, optional): Maximum sequence length for actions
            transform (str): Transform to apply to images - "id", "crop", "jitter", or "crop_jitter"
            env: Pre-created robosuite environment to use for rendering
        """
        super().__init__()
        self.demo_indices = demo_indices
        self.norm_stats = norm_stats
        self.args = args
        self.image_size = 256  # Standard image size
        self.use_plucker = args.use_plucker
        self.num_cameras = args.num_side_cam
        
        if not self.args.default_cam:
            poses_path = os.path.join(self.args.camera_poses_dir, camera_poses_file)
            with open(poses_path, 'r') as f:
                raw = json.load(f)
            self.camera_poses = raw['poses']
            
            print(f"Loaded {len(self.camera_poses)} camera poses (old format) from {poses_path}; num_side_cam={self.num_cameras}")
        else:
            # self.camera_poses = None
            print("Using default agentview camera pose (duplicated if multiple cams)")
        
        if self.use_plucker:
            self.plucker_embedder = PluckerEmbedder(img_size=self.image_size, device='cuda')
        else:
            self.plucker_embedder = None
        
        # Load demonstration data
        self.demo_states = []
        self.demo_actions = []
        self.demo_lengths = []
        
        print(f"Loading robosuite data for {len(demo_indices)} demos from {args.dataset_path}...")
        with h5py.File(args.dataset_path, "r") as dataset_file:
            for idx in self.demo_indices:
                demo_key = f'demo_{idx}'
                
                states = dataset_file[f'data/{demo_key}/states'][()].astype(np.float32)
                actions = dataset_file[f'data/{demo_key}/actions'][()].astype(np.float32)
                
                self.demo_states.append(states)
                self.demo_actions.append(actions)
                demo_len = len(actions)
                self.demo_lengths.append(demo_len)

        print(f"Successfully loaded {len(self.demo_indices)} robosuite demonstrations.")

        self.env = env

        if max_seq_length is None:
            self.max_seq_length = max(self.demo_lengths)
        else:
            self.max_seq_length = max_seq_length

        # Set up image transforms
        if transform == "id":
            self.transforms = T.Resize(self.image_size)
        elif transform == "crop":
            self.transforms = RandomCrop(min_side=int(self.image_size*0.8), max_side=self.image_size, output_size=self.image_size)
        elif transform == "crop_jitter":
            self.transforms = T.Compose([
                RandomCrop(min_side=int(self.image_size*0.8), max_side=self.image_size, output_size=self.image_size),
                RGBJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1)
            ])
        else:
            raise ValueError("Invalid transform type. Choose 'id', 'crop', 'jitter', or 'crop_jitter'.")
            
    def __len__(self):
        return len(self.demo_indices)
    
    def _get_camera_intrinsics(self):
        cam_name = "agentview"  # assume same intrinsics
        cam_id = self.env.sim.model.camera_name2id(cam_name)
        
        fovy = self.env.sim.model.cam_fovy[cam_id] * np.pi / 180.0
        width, height = self.image_size, self.image_size
        
        focal_length = height / (2 * np.tan(fovy / 2))
        
        intrinsics = np.array([
            [focal_length, 0, width / 2],
            [0, focal_length, height / 2],
            [0, 0, 1]
        ], dtype=np.float32)
        
        return intrinsics
    
    def _set_camera_pose(self, cam_to_world):
        cam_name = "agentview"
        cam_id = self.env.sim.model.camera_name2id(cam_name)
        
        self.env.sim.model.cam_pos[cam_id] = cam_to_world[:3, 3]
        rotation = Rotation.from_matrix(cam_to_world[:3, :3])
        quat = rotation.as_quat()  # [x, y, z, w]

        self.env.sim.model.cam_quat[cam_id] = [quat[3], quat[0], quat[1], quat[2]]
    
    def __getitem__(self, demo_idx):
        demo_length = self.demo_lengths[demo_idx]
        states = self.demo_states[demo_idx]
        actions = self.demo_actions[demo_idx]
        start_ts = np.random.randint(demo_length)

        self.env.sim.set_state_from_flattened(states[start_ts])

        cam_images = []
        if not self.args.default_cam:
            start = self.args.m * demo_idx
            end = start + self.args.n
            window = np.arange(start, end, dtype=np.int64)
            chosen = np.random.choice(window, size=self.args.num_side_cam, replace=False)
            pose_set = [self.camera_poses[i] for i in chosen.tolist()]
        else:
            pose_set = [None] * self.args.num_side_cam

        for cam_pose_raw in pose_set:
            if not self.args.default_cam:
                cam_pose = np.array(cam_pose_raw)
                self._set_camera_pose(cam_pose)
            self.env.sim.forward()
            rgb_img = self.env.sim.render(camera_name="agentview", height=self.image_size, width=self.image_size, depth=False)
            rgb_img = np.flipud(rgb_img).copy()
            rgb_tensor = einops.rearrange(torch.from_numpy(rgb_img).float() / 255.0, 'h w c -> c h w').cuda()

            if self.use_plucker and not self.args.default_cam:
                intrinsics = self._get_camera_intrinsics()
                intrinsics_tensor = torch.from_numpy(intrinsics).unsqueeze(0).float().cuda()
                cam_to_world_tensor = torch.from_numpy(cam_pose).unsqueeze(0).float().cuda()
                with torch.no_grad():
                    plucker_data = self.plucker_embedder(intrinsics_tensor, cam_to_world_tensor)
                    plucker_tensor = einops.rearrange(plucker_data['plucker'][0], 'h w c -> c h w')
            else:
                plucker_tensor = torch.zeros(6, rgb_tensor.shape[1], rgb_tensor.shape[2], device='cuda')
                
            img_chw = torch.cat([rgb_tensor, plucker_tensor], dim=0)
            cam_images.append(self.transforms(img_chw))

        # Stack per-camera images: [num_cameras, C, H, W]
        image_tensor = torch.stack(cam_images, dim=0)
        
        # Camera extrinsics tokens: always 2 entries [2, 4, 4]
        if self.args.use_cam_pose and not self.args.default_cam:
            cam_extrinsics_list = []
            for i in range(2):
                if i < len(pose_set) and pose_set[i] is not None:
                    cam_pose_mat = np.array(pose_set[i], dtype=np.float32)
                    cam_extrinsics_list.append(torch.from_numpy(cam_pose_mat).float().cuda())
                else:
                    cam_extrinsics_list.append(torch.zeros(4, 4, device='cuda'))
            cam_extrinsics = torch.stack(cam_extrinsics_list, dim=0)
        else:
            cam_extrinsics = torch.zeros(2, 4, 4, device='cuda')
        
        # Normalize and convert to tensors
        robot_qpos = states[start_ts][:7]
        if np.random.rand() < self.args.prob_drop_proprio:
            robot_qpos = np.zeros_like(robot_qpos)
        actions_seq = actions[start_ts:]
        
        padded_actions = np.zeros((self.max_seq_length, actions.shape[1]), dtype=np.float32)
        seq_length = min(len(actions_seq), self.max_seq_length)
        padded_actions[:seq_length] = actions_seq[:seq_length]
        
        is_pad = np.zeros(self.max_seq_length, dtype=np.bool_)
        is_pad[seq_length:] = True
        
        state_normalized = (robot_qpos - self.norm_stats["state_mean"]) / self.norm_stats["state_std"]
        actions_normalized = (padded_actions - self.norm_stats["action_mean"]) / self.norm_stats["action_std"]
        return {
            'image': image_tensor,
            'qpos': torch.from_numpy(state_normalized).float().cuda(), 
            'actions': torch.from_numpy(actions_normalized).float().cuda(),
            'is_pad': torch.from_numpy(is_pad).cuda(),
            'cam_extrinsics': cam_extrinsics
        }

    def __del__(self):
        """Close the environment when the dataset is destroyed."""
        if hasattr(self, 'env') and self.env is not None:
            self.env.close()



class EpisodicImplicitExtrinsicDataset(Dataset):
    """
    Dataset for loading episodic data from robosuite demonstration files.
    Renders images on-the-fly with dynamic camera poses and Plucker embeddings.
    """
    def __init__(self, demo_indices, norm_stats, args, camera_poses_file=None,
                 max_seq_length=None, transform="id", env=None, num_dynamic_feature=None, window_size=None, preprocess_model=None):
        """
        Args:
            demo_indices (list): List of demonstration indices to use
            norm_stats (dict): Normalization statistics for actions and states
            args: Arguments object containing dataset_path, camera_names, use_plucker, etc.
            camera_poses_file (str): Path to JSON file with camera poses
            max_seq_length (int, optional): Maximum sequence length for actions
            transform (str): Transform to apply to images - "id", "crop", "jitter", or "crop_jitter"
            env: Pre-created robosuite environment to use for rendering
        """
        super().__init__()
        self.demo_indices = demo_indices
        self.norm_stats = norm_stats
        self.args = args
        self.image_size = 256  # Standard image size
        self.use_plucker = args.use_plucker
        self.num_cameras = args.num_side_cam
        self.num_dynamic_feature = num_dynamic_feature
        self.window_size = window_size
        self.preprocess_model = preprocess_model
        if not self.args.default_cam:
            poses_path = os.path.join(self.args.camera_poses_dir, camera_poses_file)
            with open(poses_path, 'r') as f:
                raw = json.load(f)
            self.camera_poses = raw['poses']
            
            print(f"Loaded {len(self.camera_poses)} camera poses (old format) from {poses_path}; num_side_cam={self.num_cameras}")
        else:
            # self.camera_poses = None
            print("Using default agentview camera pose (duplicated if multiple cams)")
        
        if self.use_plucker:
            self.plucker_embedder = PluckerEmbedder(img_size=self.image_size, device='cuda')
        else:
            self.plucker_embedder = None
        
        # Load demonstration data
        self.demo_states = []
        self.demo_actions = []
        self.demo_lengths = []
        
        print(f"Loading robosuite data for {len(demo_indices)} demos from {args.dataset_path}...")
        with h5py.File(args.dataset_path, "r") as dataset_file:
            for idx in self.demo_indices:
                demo_key = f'demo_{idx}'
                
                states = dataset_file[f'data/{demo_key}/states'][()].astype(np.float32)
                actions = dataset_file[f'data/{demo_key}/actions'][()].astype(np.float32)
                
                self.demo_states.append(states)
                self.demo_actions.append(actions)
                demo_len = len(actions)
                self.demo_lengths.append(demo_len)

        print(f"Successfully loaded {len(self.demo_indices)} robosuite demonstrations.")

        self.env = env

        if max_seq_length is None:
            self.max_seq_length = max(self.demo_lengths)
        else:
            self.max_seq_length = max_seq_length

        # Set up image transforms
        if transform == "id":
            self.transforms = T.Resize(self.image_size)
        elif transform == "crop":
            self.transforms = RandomCrop(min_side=int(self.image_size*0.8), max_side=self.image_size, output_size=self.image_size)
        elif transform == "crop_jitter":
            self.transforms = T.Compose([
                RandomCrop(min_side=int(self.image_size*0.8), max_side=self.image_size, output_size=self.image_size),
                RGBJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1)
            ])
        else:
            raise ValueError("Invalid transform type. Choose 'id', 'crop', 'jitter', or 'crop_jitter'.")
            
    def __len__(self):
        return len(self.demo_indices)
    
    def _get_camera_intrinsics(self):
        cam_name = "agentview"  # assume same intrinsics
        cam_id = self.env.sim.model.camera_name2id(cam_name)
        
        fovy = self.env.sim.model.cam_fovy[cam_id] * np.pi / 180.0
        width, height = self.image_size, self.image_size
        
        focal_length = height / (2 * np.tan(fovy / 2))
        
        intrinsics = np.array([
            [focal_length, 0, width / 2],
            [0, focal_length, height / 2],
            [0, 0, 1]
        ], dtype=np.float32)
        
        return intrinsics
    
    def _set_camera_pose(self, cam_to_world):
        cam_name = "agentview"
        cam_id = self.env.sim.model.camera_name2id(cam_name)
        
        self.env.sim.model.cam_pos[cam_id] = cam_to_world[:3, 3]
        rotation = Rotation.from_matrix(cam_to_world[:3, :3])
        quat = rotation.as_quat()  # [x, y, z, w]

        self.env.sim.model.cam_quat[cam_id] = [quat[3], quat[0], quat[1], quat[2]]
    
    def __getitem__(self, demo_idx):
        demo_length = self.demo_lengths[demo_idx]
        states = self.demo_states[demo_idx]
        actions = self.demo_actions[demo_idx]
        cam_images = []
        future_cam_images = []
        dynamic_actions = []
        cam_extrinsic_list = []

        if not self.args.default_cam:
            start = self.args.m * demo_idx
            end = start + self.args.n
            window = np.arange(start, end, dtype=np.int64)
            chosen = np.random.choice(window, size=self.args.num_side_cam, replace=False)
            pose_set = [self.camera_poses[i] for i in chosen.tolist()]
        else:
            pose_set = [None] * self.args.num_side_cam

        if demo_length >= self.num_dynamic_feature:
            start_ts_list = np.random.choice(demo_length, size=self.num_dynamic_feature, replace=False)
        else:
            start_ts_list = np.random.choice(demo_length, size=size=self.num_dynamic_feature, replace=True)
            
        for idx in range(self.num_dynamic_feature):
            start_ts = start_ts_list[idx]

            self.env.sim.set_state_from_flattened(states[start_ts])
            self.env.set_init_action()
            dynamic_action = actions[start_ts]
            
            for cam_pose_raw in pose_set:
                if not self.args.default_cam:
                    cam_pose = np.array(cam_pose_raw)
                    self._set_camera_pose(cam_pose)
                self.env.sim.forward()
                rgb_img = self.env.sim.render(camera_name="agentview", height=self.image_size, width=self.image_size, depth=False)
                rgb_img = np.flipud(rgb_img).copy()
                rgb_tensor = einops.rearrange(torch.from_numpy(rgb_img).float() / 255.0, 'h w c -> c h w').cuda()

                if self.use_plucker and not self.args.default_cam:
                    intrinsics = self._get_camera_intrinsics()
                    intrinsics_tensor = torch.from_numpy(intrinsics).unsqueeze(0).float().cuda()
                    cam_to_world_tensor = torch.from_numpy(cam_pose).unsqueeze(0).float().cuda()
                    with torch.no_grad():
                        plucker_data = self.plucker_embedder(intrinsics_tensor, cam_to_world_tensor)
                        plucker_tensor = einops.rearrange(plucker_data['plucker'][0], 'h w c -> c h w')
                    img_chw = torch.cat([rgb_tensor, plucker_tensor], dim=0)
                else:
                    img_chw = rgb_tensor
                
                for j in range(self.window_size):
                    self.env.step(dynamic_action)

                self.env.sim.forward()
                future_rgb_img = self.env.sim.render(camera_name="agentview", height=self.image_size, width=self.image_size, depth=False)
                future_rgb_img = np.flipud(future_rgb_img).copy()
                future_rgb_tensor = einops.rearrange(torch.from_numpy(future_rgb_img).float() / 255.0, 'h w c -> c h w').cuda()

                if self.use_plucker and not self.args.default_cam:
                    future_img_chw = torch.cat([future_rgb_tensor, plucker_tensor], dim=0)
                else:
                    future_img_chw = future_rgb_tensor

                cam_images.append(img_chw)
                future_cam_images.append(future_img_chw)

            # Stack per-camera images: [num_cameras, C, H, W]
            dynamic_actions.append(dynamic_action)
            # Camera extrinsics tokens: always 2 entries [2, 4, 4]

        if self.args.use_cam_pose and not self.args.default_cam:
            cam_extrinsics_list = []
            for i in range(2):
                if i < len(pose_set) and pose_set[i] is not None:
                    cam_pose_mat = np.array(pose_set[i], dtype=np.float32)
                    cam_extrinsics_list.append(torch.from_numpy(cam_pose_mat).float().cuda())
                else:
                    cam_extrinsics_list.append(torch.zeros(4, 4, device='cuda'))
            cam_extrinsics = torch.stack(cam_extrinsics_list, dim=0)
        else:
            cam_extrinsics = torch.zeros(2, 4, 4, device='cuda')

        image_tensor = torch.stack(cam_images, dim=0)
        future_image_tensor = torch.stack(future_cam_images, dim=0)
        dynamic_actions = np.array(dynamic_actions)
        dynamic_actions_normalized = (dynamic_actions - self.norm_stats["action_mean"]) / self.norm_stats["action_std"]
        img1_224 = F.interpolate(image_tensor, size=(224,224), mode='bilinear', align_corners=False, antialias=True).clamp_(0, 1).mul_(255.0)
        img2_224 = F.interpolate(future_image_tensor, size=(224,224), mode='bilinear', align_corners=False, antialias=True).clamp_(0, 1).mul_(255.0)
        with torch.no_grad():
            self.preprocess_model.eval()
            optical_flow = self.preprocess_model.extract_flow(img1_224, img2_224)
        return {
            'image': image_tensor,
            'future_image': future_image_tensor,
            'dynamic_actions_normalized': torch.from_numpy(dynamic_actions_normalized).float().cuda(),
            'cam_extrinsics': cam_extrinsics,
            "optical_flow": optical_flow
        }

    def __del__(self):
        """Close the environment when the dataset is destroyed."""
        if hasattr(self, 'env') and self.env is not None:
            self.env.close()


# --- Data Loading Function ---

def load_data(args, env, val_split=0.1):
    with h5py.File(args.dataset_path, 'r') as f:
        available_demos = len([k for k in f['data'].keys() if k.startswith('demo_')])

    assert args.num_episodes + 10 <= available_demos, "Not enough demos to split"

    train_indices = list(range(args.num_episodes))
    val_indices = list(range(args.num_episodes, args.num_episodes + 10))
    
    print("Computing normalization statistics...")
    # Choose normalization style based on policy_class (dp -> min-max-as-mean/std)
    norm_stats = get_norm_stats(args.dataset_path, num_demos=args.num_episodes, policy_class=args.policy_class)
    print("Normalization statistics computed.")
    
    print("Loading training dataset...")
    train_dataset = EpisodicDataset(
        train_indices, 
        norm_stats, 
        args,
        camera_poses_file=args.train_poses_file,
        transform=args.transform,
        env=env
    )
    
    print("Loading validation dataset...")
    val_dataset = EpisodicDataset(
        val_indices, 
        norm_stats,
        args,
        camera_poses_file=args.test_poses_file,
        transform="id",  # Use simpler transform for validation
        env=env
    )
    print("Datasets loaded.")
    
    max_seq_length = train_dataset.max_seq_length
    print(f"Using max sequence length: {max_seq_length}")

    train_dataset.max_seq_length = max_seq_length
    val_dataset.max_seq_length = max_seq_length

    train_dataloader = DataLoader(
        train_dataset, 
        batch_size=args.batch_size, 
        shuffle=True, 
        num_workers=0,  # Disable multiprocessing due to robosuite environment
        pin_memory=False
    )
    
    val_dataloader = DataLoader(
        val_dataset, 
        batch_size=args.batch_size, 
        shuffle=False, 
        num_workers=0,
        pin_memory=False
    )
    
    return train_dataloader, val_dataloader, norm_stats


def load_implicit_extrinsic_data(args, env, val_split=0.1, preprocess_model=None):
    with h5py.File(args.dataset_path, 'r') as f:
        available_demos = len([k for k in f['data'].keys() if k.startswith('demo_')])

    assert args.num_episodes + 10 <= available_demos, "Not enough demos to split"

    train_indices = list(range(args.num_episodes))
    val_indices = list(range(args.num_episodes, args.num_episodes + 10))
    
    print("Computing normalization statistics...")
    # Choose normalization style based on policy_class (dp -> min-max-as-mean/std)
    norm_stats = get_norm_stats(args.dataset_path, num_demos=args.num_episodes, policy_class=args.policy_class)
    print("Normalization statistics computed.")
    
    print("Loading training dataset...")
    train_dataset = EpisodicImplicitExtrinsicDataset(
        train_indices, 
        norm_stats, 
        args,
        camera_poses_file=args.train_poses_file,
        transform=args.transform,
        env=env,
        num_dynamic_feature=args.num_dynamic_feature,
        window_size=args.window_size,
        preprocess_model=preprocess_model,
    )
    
    print("Loading validation dataset...")
    val_dataset = EpisodicImplicitExtrinsicDataset(
        val_indices, 
        norm_stats,
        args,
        camera_poses_file=args.test_poses_file,
        transform="id",  # Use simpler transform for validation
        env=env,
        num_dynamic_feature=args.num_dynamic_feature,
        window_size=args.window_size,
        preprocess_model=preprocess_model,

    )
    print("Datasets loaded.")
    
    max_seq_length = train_dataset.max_seq_length
    print(f"Using max sequence length: {max_seq_length}")

    train_dataset.max_seq_length = max_seq_length
    val_dataset.max_seq_length = max_seq_length

    train_dataloader = DataLoader(
        train_dataset, 
        batch_size=args.batch_size, 
        shuffle=True, 
        num_workers=0,  # Disable multiprocessing due to robosuite environment
        pin_memory=False
    )
    
    val_dataloader = DataLoader(
        val_dataset, 
        batch_size=args.batch_size, 
        shuffle=False, 
        num_workers=0,
        pin_memory=False
    )
    
    return train_dataloader, val_dataloader, norm_stats
