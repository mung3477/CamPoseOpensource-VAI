import os

import json
import time
import torch
import argparse
import tqdm
import robosuite as suite
from robosuite.wrappers.action_wrapper import wrap_env_action_space
from contextlib import nullcontext
import h5py
from pathlib import Path

from utils import load_data, compute_dict_mean, set_seed, detach_dict, constant_schedule, cleanup_ckpt, get_last_ckpt, cosine_schedule_with_warmup
from models.act import ACTPolicy
from eval import Evaluator
from models.dp import DiffusionPolicy
from models.smolvla import SmolVLAPolicyWrapper

import wandb

torch.backends.cuda.enable_flash_sdp(True)
print(torch.backends.cuda.is_flash_attention_available())

def main(args, ckpt=None):
    set_seed(args.seed)
    start_time = time.time()

    args.action_dim = 8 if 'joint' in args.dataset_path else 7

    # Build environment from dataset metadata
    with h5py.File(args.dataset_path, 'r') as f:
        env_args_raw = f['data'].attrs['env_args']
        action_space = f['data'].attrs['action_space']
    env_config = json.loads(env_args_raw)
    env_name = env_config['env_name']
    env_kwargs = env_config['env_kwargs']

    env_kwargs.update({
        'has_renderer': False,
        'has_offscreen_renderer': True,
        'use_camera_obs': False,
        'camera_heights': 256,
        'camera_widths': 256,
    })

    if args.original and 'Rand' in env_name:
        env_kwargs['original'] = True

    env = suite.make(env_name=env_name, camera_names=["agentview"], **env_kwargs)
    if action_space in ('eef_delta', 'joint_delta'):
        env = wrap_env_action_space(env, action_space)
    env.reset()

    # IMPORTANT: Import after suite.make()
    import OpenGL.GL as gl

    train_dataloader, val_dataloader, stats = load_data(
        args=args,
        env=env
    )

    # Save stats
    os.makedirs(args.ckpt_dir, exist_ok=True)
    stats_path = os.path.join(args.ckpt_dir, 'dataset_stats.json')
    with open(stats_path, 'w') as f:
        json.dump({k: v.tolist() for k, v in stats.items()}, f, indent=4)

    evaluator = Evaluator(env=env, norm_stats=stats, dataset_path=args.dataset_path, args=args)

    if args.policy_class == 'act':
        policy = ACTPolicy(args).cuda()
    elif args.policy_class == 'dp':
        policy = DiffusionPolicy(args).cuda()
    elif args.policy_class == 'smolvla':
        policy = SmolVLAPolicyWrapper(args).cuda()

    optimizer = policy.configure_optimizers()
    if args.policy_class == 'smolvla':
        steps_per_epoch = len(train_dataloader)
        total_steps = args.num_epochs * steps_per_epoch
        scheduler = cosine_schedule_with_warmup(optimizer, 0.05 * total_steps, total_steps)
    else:
        scheduler = constant_schedule(optimizer)


    epoch = 0
    if ckpt is not None:
        policy.load_state_dict(ckpt['model_state_dict'])
        if 'optimizer_state_dict' in ckpt:
            optimizer.load_state_dict(ckpt['optimizer_state_dict'])
        if 'scheduler_state_dict' in ckpt:
            scheduler.load_state_dict(ckpt['scheduler_state_dict'])
        epoch = ckpt['epoch'] + 1
        print(f"Resumed from checkpoint at epoch {ckpt['epoch']}")

    pbar = tqdm.tqdm(total=args.num_epochs, desc="Training")
    pbar.update(epoch)

    while epoch < args.num_epochs:

        # Check OpenGL framebuffer. This is a hack to fix the renderer issue in robosuite.
        if gl.GL_FRAMEBUFFER_COMPLETE != gl.glCheckFramebufferStatus(gl.GL_FRAMEBUFFER):
            print("⚠️  Render fault detected; rebuilding context")
            env.reset()

        # Validation
        if epoch % 10 == 0:
            with torch.inference_mode():
                policy.eval()
                epoch_dicts = []
                for data in val_dataloader:
                    with torch.autocast("cuda", dtype=torch.bfloat16) if args.use_fp16 else nullcontext():
                        forward_dict = policy(data)
                    epoch_dicts.append(forward_dict)
                epoch_summary = compute_dict_mean(epoch_dicts)
                for k, v in epoch_summary.items():
                    wandb.log({f'val_{k}': v.item()}, step=epoch)

        # Evaluation
        if epoch % args.eval_every == 0:
            policy.eval()
            for pose_file in args.pose_files:
                pose_name = pose_file[:-5]  # Remove .json extension
                eval_save_path = os.path.join(args.ckpt_dir, f"eval_epoch_{epoch}_{pose_name}")
                os.makedirs(eval_save_path, exist_ok=True)
                evaluator.success_by_seed = {}

                success_rates = []
                for episode_idx in range(50 if epoch > args.eval_start_epoch else args.eval_episodes):
                    with torch.no_grad():
                        _, success_rate, _ = evaluator.evaluate(
                            policy=policy,
                            save_path=eval_save_path,
                            video_prefix=f"epoch_{epoch}_episode_{episode_idx}",
                            pose_name=pose_name,
                            episode_num=episode_idx
                        )
                    success_rates.append(success_rate)
                avg_success_rate = sum(success_rates) / len(success_rates)
                wandb.log({f'success_rate_{pose_name}': avg_success_rate}, step=epoch)

                with open(os.path.join(eval_save_path, 'success_by_seed.json'), 'w') as f:
                    json.dump(evaluator.success_by_seed, f, indent=2)

        # Save checkpoint
        if epoch % args.save_every == 0:
            checkpoint_path = os.path.join(args.ckpt_dir, f'epoch_{epoch}.pth')
            torch.save({
                'epoch': epoch,
                'model_state_dict': policy.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'loss': epoch_summary['loss'],
                'wandb_id': wandb.run.id
            }, checkpoint_path)
            cleanup_ckpt(args.ckpt_dir, keep=3)  # Keep last 3 checkpoints

            # No Timeout
            # if time.time() - start_time > 7.5 * 60 * 60:
            #     print(f"⏰ Time limit reached ({(time.time() - start_time)/3600:.1f} hours). Exiting...")
            #     break

        # Check OpenGL framebuffer. This is a hack to fix the renderer issue in robosuite.
        if gl.GL_FRAMEBUFFER_COMPLETE != gl.glCheckFramebufferStatus(gl.GL_FRAMEBUFFER):
            print("⚠️  Render fault detected; rebuilding context")
            env.reset()

        # Training
        train_history = []
        policy.train()
        for data in train_dataloader:
            with torch.autocast("cuda", dtype=torch.bfloat16) if args.use_fp16 else nullcontext():
                forward_dict = policy(data)
                loss = forward_dict['loss']

            loss.backward()
            torch.nn.utils.clip_grad_norm_(policy.parameters(), max_norm=1.0)
            optimizer.step()
            wandb.log({'lr': optimizer.param_groups[0]['lr']}, step=epoch)
            optimizer.zero_grad(set_to_none=True)
            scheduler.step()
            train_history.append(detach_dict(forward_dict))

        epoch_summary = compute_dict_mean(train_history)
        for k, v in epoch_summary.items():
            wandb.log({f'train_{k}': v.item()}, step=epoch)

        pbar.update(1)
        epoch += 1

    pbar.close()
    env.close()
    wandb.finish()

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="Train ACT policy for robosuite environments")
    def str2bool(v): return str(v) == "1"

    # General config
    parser.add_argument('--name', type=str, default='oct16_open_source', help='name for the run')
    parser.add_argument('--dataset_dir', type=str, default=None,
                        help='Path to dataset directory (absolute). If None, defaults to policy_robosuite/demos')
    parser.add_argument('--dataset_suffix', type=str, default='liftrand_eef_delta')
    parser.add_argument('--dataset_path', type=str, default=None)
    parser.add_argument('--camera_poses_dir', type=str, default=None,
                        help='Path to camera poses directory (absolute). If None, defaults to policy_robosuite/camera_poses')
    parser.add_argument('--ckpt_dir', type=str, default=None,
                        help='Path to checkpoints directory (absolute). If None, defaults to policy_robosuite/checkpoints/<name>')
    parser.add_argument('--policy_class', type=str, default='act', choices=['dp','act','smolvla'], help='policy class')

    parser.add_argument('--num_episodes', default=200, type=int, help='num_episodes')
    parser.add_argument('--use_plucker', default=False, type=str2bool, help='use Plucker embeddings')
    parser.add_argument('--wandb_project_name', type=str, default='DynamicVLA', help='wandb project name')
    parser.add_argument('--wandb_entity', type=str, default=None, help='wandb entity')

    # Camera pose config
    parser.add_argument('--train_poses_file', type=str, default='train_cameras.json', help='Path to training camera poses JSON (old flat format)')
    parser.add_argument('--test_poses_file', type=str, default='test_cameras.json', help='Path to test camera poses JSON (old flat format)')
    parser.add_argument('--pose_files', nargs='+', default=['train_cameras.json', 'test_cameras.json'], help='Pose files to evaluate on (default: test only)')
    parser.add_argument('--n', type=int, default=3, help='Number of cameras per window W(i) = [m*i, m*i + n)')
    parser.add_argument('--m', type=int, default=1, help='Stride for camera window W(i) = [m*i, m*i + n)')
    parser.add_argument('--num_side_cam', type=int, default=1, choices=[1,2], help='Number of side cams to use (1 or 2)')
    parser.add_argument('--default_cam', type=str2bool, default=False, help='When true, use default agentview pose for all cameras (duplicate if >1)')

    # Training config
    parser.add_argument('--batch_size', default=70, type=int, help='batch_size')
    parser.add_argument('--seed', default=0, type=int, help='seed')
    parser.add_argument('--num_epochs', default=30_001, type=int, help='num_epochs')
    parser.add_argument('--eval_start_epoch', type=int, default=20_000, help='start evaluating 50 at this epoch')
    parser.add_argument('--lr', type=float, default=2e-5, help='lr')
    parser.add_argument('--save_every', type=int, default=1000, help='save checkpoint every N epochs')
    parser.add_argument('--use_fp16', default=True, type=str2bool, help='use mixed precision bf16 training')

    # Dataloader config
    parser.add_argument('--transform', type=str, default='crop', choices=['crop', 'id', 'crop_jitter'],
                        help='Image transformation type')
    parser.add_argument('--prob_drop_proprio', default=1., type=float, help='probability to drop proprio')
    parser.add_argument('--use_cam_pose', default=True, type=bool, help='otherwise mask to 0')
    parser.add_argument('--original', default=False, type=str2bool, help='visually same as original lift')


    # ACT model config
    parser.add_argument('--dropout', type=float, default=0.1, help='dropout')
    parser.add_argument('--latent_drop_prob', type=float, default=0.0, help='drop probability for RGB latents in backbone')
    parser.add_argument('--kl_weight', type=float, default=1.0, help='KL Weight')
    parser.add_argument('--chunk_size', type=int, default=30, help='chunk_size')
    parser.add_argument('--hidden_dim', type=int, default=512, help='hidden_dim')
    parser.add_argument('--weight_decay', type=float, default=1e-4, help='weight_decay')
    parser.add_argument('--obs_dim', type=int, default=7, help='observation dimension')


    parser.add_argument('--nheads', type=int, default=8, help='number of attention heads')
    parser.add_argument('--ffn_dim', type=int, default=2048, help='feedforward network dimension')
    parser.add_argument('--enc_layers', type=int, default=4, help='number of encoder layers')
    parser.add_argument('--dec_layers', type=int, default=7, help='number of decoder layers')
    parser.add_argument('--pre_norm', type=bool, default=True, help='use pre-normalization')
    parser.add_argument('--activation', default='relu', help='activation function')

    # Backbone config
    parser.add_argument('--backbone', default='late_imagenet', help='backbone: resnet, linear')
    parser.add_argument('--patch_size', type=int, default=8, help='patch size')

    # Evaluation config
    parser.add_argument('--eval_every', type=int, default=1000, help='evaluate every N epochs')
    parser.add_argument('--eval_episodes', type=int, default=10, help='number of evaluation episodes')
    parser.add_argument('--eval_max_steps', type=int, default=300, help='max steps per evaluation episode')
    parser.add_argument('--eval_save_n_video', type=int, default=10, help='save the first n videos for each evaluation epoch')

    # SmolVLA finetuning flags
    parser.add_argument('--freeze_vision_encoder', type=str2bool, default=False, help='freeze the vision encoder (SigLIP)')
    parser.add_argument('--train_expert_only', type=str2bool, default=False, help='train only the action expert; freeze VLM')
    args = parser.parse_args()

    group = args.name[:-7] # remove the seed from the name

    # Anchor paths under the module root (policy_robosuite)
    MODULE_ROOT = Path(__file__).resolve().parent

    # dataset_dir -> dataset_path
    if args.dataset_dir is None:
        args.dataset_dir = str((MODULE_ROOT / "demos").resolve())
    args.dataset_path = os.path.join(args.dataset_dir, args.dataset_suffix + ".hdf5")

    # camera poses dir
    if args.camera_poses_dir is None:
        args.camera_poses_dir = str((MODULE_ROOT / "camera_poses").resolve())
    if not hasattr(args, 'ckpt_dir') or args.ckpt_dir is None:
        args.ckpt_dir = str((MODULE_ROOT / "checkpoints" / args.name).resolve())
    os.makedirs(args.ckpt_dir, exist_ok=True)

    # Save config
    config_path = os.path.join(args.ckpt_dir, 'config.json')
    with open(config_path, 'w') as f:
        json.dump(vars(args), f, indent=4)

    # Check for existing checkpoint to resume
    ckpt_path = get_last_ckpt(args.ckpt_dir)

    if ckpt_path is not None:
        print(f"Resuming from checkpoint: {ckpt_path}")
        ckpt = torch.load(ckpt_path)
        wandb_id = ckpt['wandb_id']
        wandb.init(entity=args.wandb_entity, project=args.wandb_project_name, id=wandb_id, resume='must', group=group)
        main(args, ckpt)
    else:
        print(f"Starting new training run: {args.name}")
        wandb.init(entity=args.wandb_entity, project=args.wandb_project_name, name=args.name, config=vars(args), group=group)
        main(args)
